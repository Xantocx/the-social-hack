{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************************************************************\n",
    "#  The Social Web \n",
    "- Instructors: Davide Ceolin.\n",
    "- TAs: Jacco van Ossenbruggen, Elena Beretta, Mirthe Dankloff.\n",
    "- Exercises for Hands-on session 1- TAs: Jacco van Ossenbruggen, Elena Beretta, Mirthe Dankloff.\n",
    "*****************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites:\n",
    "- Python 3.8\n",
    "- Python packages: twitter, prettytable, matplotlib\n",
    "\n",
    "First you need to know how to retrieve some social web data. Exercises 1 and 2 will show you how to retrieve trends and search results from Twitter. \n",
    "\n",
    "But let's check first if we're running a sufficiently new version of Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This jupyter notebook is running on Python 3.8.5\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "print(\"This jupyter notebook is running on Python \" + platform.python_version())\n",
    "# It's good practice to assert packages requirements at the beginning of a script:\n",
    "assert sys.version_info >= (3, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install now the required packages for this hands on session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-twitter-v2 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (0.7.9)\n",
      "Requirement already satisfied: PrettyTable in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: geocoder in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (1.38.1)\n",
      "Requirement already satisfied: tweepy in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (4.12.1)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.2 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from python-twitter-v2) (0.5.7)\n",
      "Requirement already satisfied: Authlib<0.16.0,>=0.15.4 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from python-twitter-v2) (0.15.6)\n",
      "Requirement already satisfied: requests<3.0,>=2.24 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from python-twitter-v2) (2.28.1)\n",
      "Requirement already satisfied: wcwidth in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from PrettyTable) (0.2.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from matplotlib) (1.23.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: future in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from geocoder) (0.18.2)\n",
      "Requirement already satisfied: click in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from geocoder) (8.1.3)\n",
      "Requirement already satisfied: ratelim in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from geocoder) (0.1.6)\n",
      "Requirement already satisfied: six in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from geocoder) (1.16.0)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: cryptography in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from Authlib<0.16.0,>=0.15.4->python-twitter-v2) (38.0.3)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.2->python-twitter-v2) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.2->python-twitter-v2) (3.18.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.2->python-twitter-v2) (0.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from requests<3.0,>=2.24->python-twitter-v2) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from requests<3.0,>=2.24->python-twitter-v2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from requests<3.0,>=2.24->python-twitter-v2) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from requests<3.0,>=2.24->python-twitter-v2) (2022.9.24)\n",
      "Requirement already satisfied: colorama in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from click->geocoder) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from ratelim->geocoder) (5.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.2->python-twitter-v2) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.2->python-twitter-v2) (4.4.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from cryptography->Authlib<0.16.0,>=0.15.4->python-twitter-v2) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\iw\\python\\projects\\.virtualevns\\soweb\\lib\\site-packages (from cffi>=1.12->cryptography->Authlib<0.16.0,>=0.15.4->python-twitter-v2) (2.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'C:\\IW\\python\\projects\\.virtualevns\\soweb\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# If you're using a virtualenv, make sure its activated before running \n",
    "# this cell!\n",
    "!pip install python-twitter-v2 PrettyTable matplotlib geocoder tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Authorizing an application to access Twitter account data:\n",
    "\n",
    "1. Make sure to add your mobile phone number to your private twitter profile.\n",
    "2. Go to https://developer.twitter.com/en/portal/dashboard and click on \"create an app\". Twitter will prompt you to create a *developer account*.\n",
    "3. You'll receive an *account confirmation* email with a link. Follow it and create an app. \n",
    "4. Once the app is created, you'll see a \"Keys and Token\" item on the top right tab of the webpage. These values will be needed to fill in the next cell.\n",
    "5. Please delete all your keys before submission.\n",
    "\n",
    "NOTE: This notebook is based on the API functionalities \"Essential\" license. Check the \"Academic Research\" license (right menu, \"Twitter API v2\", \"Academic Research\") if you need a broader set of functionalities.\n",
    "\n",
    "In this notebook, you can use the official Twitter V2 API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytwitter import Api \n",
    "\n",
    "TWITTER_API_KEY = \"kiykuL1dcrZaDBu87AJ6B6qYJ\"\n",
    "TWITTER_API_SECRET = \"e2jVRp2NYUASVnzPMoDlqSF1mCXg2sn2Wk5ji5m7wxaRwEa8B5\"\n",
    "# to get the oauth credential you need to click on the 'Generate access token' button:\n",
    "TWITTER_ACCESS_TOKEN = \"342328729-r299Soto2ws05aoFJGf5VF5W5WjuT9kiOexMQkIg\"\n",
    "TWITTER_ACCESS_SECRET = \"pPiRurJ0Qafakv6eukLh2RdFpyE8q0v1UpQVMPOLbrFjy\"\n",
    "\n",
    "api = Api(\n",
    "        consumer_key=TWITTER_API_KEY,\n",
    "        consumer_secret=TWITTER_API_SECRET,\n",
    "        access_token=TWITTER_ACCESS_TOKEN,\n",
    "        access_secret=TWITTER_ACCESS_SECRET\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the Tweepy API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_API_SECRET)\n",
    "auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_SECRET)\n",
    "\n",
    "twapi = tweepy.API(auth)\n",
    "\n",
    "client = tweepy.Client(bearer_token=\"AAAAAAAAAAAAAAAAAAAAANsrjAEAAAAA2khjQ8naqwRDv71DnABFiWMFVt4%3DQUdWxiq7UoeqxvvqQHwZOpvL7sEM3rXusB0cs5dTV7MipoYlrH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Retrieving twitter search trends [Only with Academic Research License]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geocoder\n",
    "\n",
    "# loc = \"Amsterdam\"\n",
    "\n",
    "# g = geocoder.osm(loc)\n",
    "# closest_loc = twapi.closest_trends(g.lat, g.lng)\n",
    "# trends = twapi.get_place_trends(closest_loc[0][\"woeid\"])\n",
    "\n",
    "# print(trends[0][\"trends\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 [Only with Academic Research License]\n",
    "Twitter uses WOEIDs. Find out how WORLD_WOE_IDs were originally defined by Yahoo! and try to use others in a query. What kind of differences do you find between the worldwide trends and the local trends? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Retrieving recent Tweets [Any license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(data=[Tweet(id=1589640456396099584, text=#TwitterOFF), Tweet(id=1589631621606834176, text=RT @tefylove2020: https://t.co/t24aJfz3Fp...), Tweet(id=1589627689396502529, text=#twitteroff https://t.co/t8SBYV3Gyv), Tweet(id=1589619170278531074, text=#TwitterOff), Tweet(id=1589613815335125000, text=RT @Camille_Jeann: #TwitterOFF...), Tweet(id=1589596669867601920, text=Ø¬Ù…Ø§Ù‡ÙŠØ± Ù„ÙŠÙØ±Ø¨ÙˆÙ„ Ø¨Ø¹Ø¯ Ø®Ø³Ø§Ø±ØªÙ‡Ù… Ø¶Ø¯ Ø§Ù„Ø±ÙŠØ§Ù„ #Twitteroff), Tweet(id=1589594688151252992, text=BD mis pequeÃ±ajos deseo que sigan pasando y...), Tweet(id=1589589066181677056, text=RT @Capitano_M14: Ø§Ø³ÙˆØ¡ ØªØ§ÙŠÙ… Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø·Ù„Ø§Ù‚ Ù‡Ùˆ ØªØ§ÙŠÙ…...), Tweet(id=1589581033766547457, text=#twitteroff ÙŠÙ…ÙƒÙ† Ù…Ø¹Ø¯ Ø§Ø±Ø¬Ø¹ ðŸ˜¢)])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "q = '#twitteroff' # XXX: Set this variable to a trending topic, or anything else you like. \n",
    "search_results = api.search_tweets(q)\n",
    "\n",
    "# The following code allows you to print in a nice format the contents of search_results\n",
    "pprint.pprint(search_results, depth=1, width=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "In the cell below, create a second variable (e.g. `statuses2`) that holds the results of a query other than the one presented above. Think about a query that would yield very different results than the first one, for example one that may yield a shorter output or about a different topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "q = '#twitteroff' # XXX: Set this variable to a trending topic, or anything else you like. \n",
    "\n",
    "# statuses2 = api.search_tweets(q)\n",
    "\n",
    "\n",
    "# search_results = client.search_all_tweets(q) \n",
    "statuses2 = twapi.mentions_timeline(count=10, include_entities=True)\n",
    "\n",
    "# The following code allows you to print in a nice format the contents of search_results\n",
    "# pprint.pprint(search_results, depth=1, width=1)#\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Extracting text, screen names, and hashtags from tweets \n",
    "\n",
    "Simply printing all the search results to screen is nice, but to really start analysing them, it is handy to select the interesting parts and store them in a different structure such as a list. \n",
    "\n",
    "In this example you are using a thing called \"List Comprehension\".\n",
    "\n",
    "### 2.1 List Comprehensions\n",
    "List comprehension is a powerful construct that allows to succinctly build a list.\n",
    "With it you can process items from any iterable (e.g. dictionaries, lists, tuples, iterators...) and output a list while optionally performing an operation on each value.\n",
    "\n",
    "Here's a few examples from Mining the Social Web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
      "[1, 9, 25, 49, 81]\n",
      "['The', 'Social', 'Web']\n",
      "[3, 6, 3]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# double all values from 0 to 9\n",
    "double_list = [i*2 for i in range(10)]\n",
    "\n",
    "# raise to the power of 2, but only if the number is uneven\n",
    "power_even_list = [i**2 for i in range(10) if i%2!=0]\n",
    "\n",
    "# clean strings in a tuple\n",
    "stripped_lines = [x.strip() for x in ('The\\n', 'Social\\n', 'Web\\n')]\n",
    "\n",
    "# return length of each string in stripped_lines\n",
    "len_str_lines = [len(s) for s in stripped_lines]\n",
    "\n",
    "# finally, we can nest list comprehensions to flatten a list of lists:\n",
    "list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "range_9 = [x for y in list_of_lists for x in y]\n",
    "\n",
    "print(double_list)\n",
    "print(power_even_list)\n",
    "print(stripped_lines)\n",
    "print(len_str_lines)\n",
    "print(range_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parsing text, screen names and hashtags from tweets\n",
    "*(from Example 1-6 in Mining the Social Web)*\n",
    "\n",
    "Hereafter, we'll be creating a variable `status_texts` of type list. \\\n",
    "The list will be filled with the `text` elements from each `status`, whereas `status` comes from looping through all `statuses` in the `search_results` list (1.2). \\\n",
    "Look up the list comprehensions in your Python reference materials to make sure you understand what's happening here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Response' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [213], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m status_texts \u001b[39m=\u001b[39m [ search_result\u001b[39m.\u001b[39mid \u001b[39mfor\u001b[39;00m search_result \u001b[39min\u001b[39;00m search_results ]\n\u001b[0;32m      3\u001b[0m \u001b[39m# the escape character \"\\\" allows for the list comprehension to continue\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# on a new line. While not strictly necessary, it makes code more readable\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# for your fellow programmers.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m status_texts \u001b[39m=\u001b[39m [ search_result\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m search_result \u001b[39min\u001b[39;00m search_results ]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Response' object is not iterable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "status_texts = [ search_result.id for search_result in search_results ]\n",
    "# the escape character \"\\\" allows for the list comprehension to continue\n",
    "# on a new line. While not strictly necessary, it makes code more readable\n",
    "# for your fellow programmers.\n",
    "\n",
    "status_texts = [ search_result.text for search_result in search_results ]\n",
    "\n",
    "#hashtags = [ hashtag['text'] for status in statuses \\\n",
    "#        for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w for t in status_texts for w in t.split() ] #split the string on the empty spaces\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "print(json.dumps(status_texts, indent=1))\n",
    "print(json.dumps(words, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "You are now ready to parse usernames, hashtags and text from the results you previously obtained in Task 2 (e.g. `statuses_2`). While doing it, make sure to leave the variables created in 2.2 untouched. Instead, create your own variable names, which you'll be using soon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'hashtags'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [214], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m usernames \u001b[39m=\u001b[39m [ status\u001b[39m.\u001b[39muser\u001b[39m.\u001b[39mscreen_name \u001b[39mfor\u001b[39;00m status \u001b[39min\u001b[39;00m statuses2 ] \n\u001b[1;32m----> 2\u001b[0m hashtags \u001b[39m=\u001b[39m [ status\u001b[39m.\u001b[39mentities\u001b[39m.\u001b[39mhashtags \u001b[39mfor\u001b[39;00m status \u001b[39min\u001b[39;00m statuses2 ]\n\u001b[0;32m      3\u001b[0m text \u001b[39m=\u001b[39m [ status\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m status \u001b[39min\u001b[39;00m statuses2 ]\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(usernames)\n",
      "Cell \u001b[1;32mIn [214], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m usernames \u001b[39m=\u001b[39m [ status\u001b[39m.\u001b[39muser\u001b[39m.\u001b[39mscreen_name \u001b[39mfor\u001b[39;00m status \u001b[39min\u001b[39;00m statuses2 ] \n\u001b[1;32m----> 2\u001b[0m hashtags \u001b[39m=\u001b[39m [ status\u001b[39m.\u001b[39;49mentities\u001b[39m.\u001b[39;49mhashtags \u001b[39mfor\u001b[39;00m status \u001b[39min\u001b[39;00m statuses2 ]\n\u001b[0;32m      3\u001b[0m text \u001b[39m=\u001b[39m [ status\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m status \u001b[39min\u001b[39;00m statuses2 ]\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(usernames)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'hashtags'"
     ]
    }
   ],
   "source": [
    "usernames = [ status.user.screen_name for status in statuses2 ] \n",
    "hashtags = [ status.entities['hashtags'] for status in statuses2 ]\n",
    "text = [ status.text for status in statuses2 ]\n",
    "print(usernames)\n",
    "print(hashtags)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating a basic frequency distribution from words in tweets\n",
    "*(from Examples 1-7 in Mining the Social Web)* \n",
    "\n",
    "\n",
    "In the cell below we display the 10 most common hashtag instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for item in [words]:\n",
    "    c = Counter(item)\n",
    "    \n",
    "print(c.most_common()[:10]) # top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look something like this: \\\n",
    "`[('ThrowbackThursday', 34), ('throwbackthursday', 11), ('TBT', 6), ('ThrowBackThursday', 6), ('Trivia', 3), ('madoka_magica', 2), ('New', 2), ('EURO2020', 2), ('artists', 2)]`\n",
    "\n",
    "### Task 4\n",
    "Show hashtags frequency for results that you obtained in Task 3. Think about possible explanations for the different results you get from the analyses for the different queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Storing your results\n",
    "So far, we have been storing the data in working memory. Often it's handy to store your data to disk so you can retrieve it in a next session. \n",
    "\n",
    "The pickle module lets you do exactly that, by serializing data in a binary format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filepath = \"my_data.pickle\"\n",
    "# this indented python syntax is broadly defined as \"context manager\".\n",
    "# This means that everything happening under its indentation will use f\n",
    "# as file handle to filepath. The Shortand `wb` stands for \"write binary\",\n",
    "# which is how we serialize data to disk.\n",
    "with open(filepath, \"wb\") as f:\n",
    "    pickle.dump(words, f) # write the contents of list 'words' to file 'f'\n",
    "    \n",
    "# Note that, after the end of the indented block, the file is automatically closed.\n",
    "# Hence, no memory resource on your system is wasted idly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you browse to your working directory, you should find a file there named \"myData.pickle\". You can open this in a text editor, or load its contents back into a variable to do some more analyses on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the myData.pickle file and store its contents into variable 'words'\n",
    "\n",
    "with open(filepath, \"rb\") as f:\n",
    "    words = pickle.load(f)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using prettytable to display tuples in a nice way\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "pt = PrettyTable(field_names=['Words', 'Count'])\n",
    "c = Counter(words)\n",
    "[ pt.add_row(kv) for kv in c.most_common()[:10] ]\n",
    "pt.align[\"Words\"], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "print(pt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Calculating lexical diversity for tweets \n",
    "*(from Example 1-9 in Mining the Social Web)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for computing lexical diversity\n",
    "def lexical_diversity(tokens):\n",
    "    return 1.0*len(set(tokens))/len(tokens)\n",
    "\n",
    "# Define a function for computing the average number of words per tweet\n",
    "def average_words(statuses):\n",
    "    total_words = sum([ len(s.split()) for s in status_texts ])\n",
    "    return 1.0*total_words/len(statuses) \n",
    "\n",
    "# Let's use these functions:\n",
    "\n",
    "print(lexical_diversity(words))\n",
    "print(average_words(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: What do the printed numbers indicate? Try to explain them.\n",
    "\n",
    "(*Double click this cell to write your answer*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Looking up users who have retweeted a status \n",
    "*(from Example 1-11 in Mining the Social Web):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = client.get_retweeters(id=1224140327688249349) # Get the original tweet id for a tweet from its retweeted_status node and insert it here\n",
    "print(\"Users who've retweeted the tweet:\\n\")\n",
    "print([retweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (advanced)\n",
    "\n",
    "If you have a Twitter account with a nontrivial number of tweets, request your historical tweet archive from your account settings and analyze it. \\\n",
    "The export of your account data includes files organized by time period in a convenient JSON format. See the README.txt file included in the downloaded archive for more details. \n",
    "\n",
    "\n",
    "\n",
    "What are the most common terms that appear in your tweets? \\\n",
    "Who do you retweet the most often? \\\n",
    "How many of your tweets are retweeted (and why do you think this is the case)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Plotting frequencies of words \n",
    "*(from Example 1-12 in Mining the Social Web)*\n",
    "\n",
    "\n",
    "In the previous exercises we have been looking at the text from the tweets, but when you retrieved the results, you retrieved much more information about the tweets, such as the username of the person who shared this tweet with the world. \n",
    "\n",
    "\n",
    "You can use this information to find out who retweets whom in our examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = sorted(Counter(words).values(), reverse=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.loglog(word_counts)\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.xlabel(\"Word Rank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating histograms of words, screen names, and hashtags \n",
    "*(from Example 1-13 in Mining the Social Web):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(words)\n",
    "plt.hist(c.values())\n",
    "    \n",
    "plt.title(\"\")\n",
    "plt.ylabel(\"Number of items in bin\")\n",
    "plt.xlabel(\"Bins (number of times an item appeared)\")\n",
    "    \n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra: seaborn plots with a one-liner.\n",
    "!pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(word_counts, kde=False, rug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('soweb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e2f54a07a7aa306880957be3bace031e4ba51b0e0cea72432f5f55b1968334"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
